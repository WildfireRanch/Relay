# File: agents/codex_agent.py
# Purpose: Handle code-editing tasks like bugfixes, refactors, or docstring generation
# Usage: Called by /ask or other routes when a code patch is requested
# Response includes a natural language summary and a structured "patch" action
# Dependencies: OpenAI (GPT-4), Relay config, project file context

import os
from typing import Dict, Any, Optional, Tuple, AsyncGenerator
from openai import AsyncOpenAI, OpenAIError
from utils.openai_client import create_openai_client
from utils.patch_utils import validate_patch_format
from core.logging import log_event
from dotenv import load_dotenv
from agents.critic_agent import run_critics

load_dotenv()

client = create_openai_client()

# === CodexAgent Main Handler ===
async def handle(message: str, context: str, user_id: Optional[str] = None) -> Dict[str, Any]:
    """
    Handles natural language prompts to perform code editing tasks.

    Args:
        message (str): Natural language request like "Fix this bug".
        context (str): Relevant file or code snippet text.
        user_id (str, optional): For logging or multi-tenant use.

    Returns:
        Dict with:
            - response: natural language summary of change
            - action: { type: "patch", target_file, patch, reason }
            - critics: List of structure/logic/safety outputs
    """
    if not message or not context:
        raise ValueError("Both message and context must be provided.")

    prompt = build_prompt(message, context)

    try:
        completion = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a senior software engineer generating code patches from user requests."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
    except OpenAIError as e:
        log_event("codex_agent_error", {"error": str(e), "user_id": user_id})
        raise RuntimeError("Codex agent failed to respond.") from e

    content = completion.choices[0].message.content.strip()
    response, action = parse_codex_response(content)

    if not validate_patch_format(action):
        raise ValueError("Invalid patch format returned from Codex.")

    # Build pseudo-plan and run critics
    pseudo_plan = {
        "objective": action["reason"],
        "steps": [f"Apply patch to {action['target_file']}"],
        "recommendation": "Generated by Codex Agent"
    }

    try:
        critics = await run_critics(pseudo_plan, context)
        action["critics"] = critics
        log_event("codex_patch_critique", {
            "file": action["target_file"],
            "user_id": user_id,
            "critics": critics
        })
    except Exception as critic_error:
        log_event("codex_critic_fail", {
            "error": str(critic_error),
            "user_id": user_id,
            "target_file": action["target_file"]
        })
        action["critics"] = [{"name": "system", "passes": False, "issues": ["Critic system failed"]}]

    log_event("codex_agent_success", {"action": action, "user_id": user_id})

    return {
        "response": response,
        "action": action
    }


# === Streaming Variant ===
async def stream(message: str, context: str, user_id: Optional[str] = None) -> AsyncGenerator[str, None]:
    """Stream CodexAgent patch response line by line."""
    if not message or not context:
        yield "[Error] Missing message or context."
        return

    prompt = build_prompt(message, context)

    try:
        openai_stream = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a senior software engineer generating code patches from user requests."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            stream=True
        )
        async for chunk in openai_stream:
            delta = chunk.choices[0].delta.content
            if delta:
                yield delta
    except Exception as e:
        yield f"[Error] Codex stream failed: {str(e)}"


# === Prompt Builder ===
def build_prompt(message: str, context: str) -> str:
    return f"""
You will receive a code editing request from a user, along with the relevant code context.

Task: {message}

Code Context:
```python
{context}
```

Please return:
1. A brief natural language summary of the change
2. A code patch (diff-style or full replacement), targeting a specific file
3. A short reason explaining why the patch is needed

Output format:
```
Summary: <text>
File: <relative/path/to/file.py>
Patch:
<diff or full replacement>
Reason: <text>
```
"""


# === Response Parser ===
def parse_codex_response(content: str) -> Tuple[str, Dict[str, str]]:
    lines = content.strip().splitlines()
    summary = next((line.replace("Summary:", "").strip() for line in lines if line.startswith("Summary:")), "Edit generated.")
    file_line = next((line for line in lines if line.startswith("File:")), "")
    file_path = file_line.replace("File:", "").strip()

    patch_lines = []
    reason = "No reason provided."
    in_patch = False

    for line in lines:
        if line.startswith("Patch:"):
            in_patch = True
            continue
        if line.startswith("Reason:"):
            reason = line.replace("Reason:", "").strip()
            break
        if in_patch:
            patch_lines.append(line)

    return summary, {
        "type": "patch",
        "target_file": file_path,
        "patch": "\n".join(patch_lines).strip(),
        "reason": reason
    }


# === TODOs for Future ===
# - Validate patch syntax or apply dry-run (AST-based)
# - Add preview rendering endpoint (/preview/patch)
# - Allow multi-file actions (currently only supports 1 file)
# - Add agent self-critique / fallback suggestion
# - Enable voting/scoring system for multiple patch candidates
# - Integrate external lint/static analysis (ruff, ESLint, etc.)
# - Surface top N critic issues in the UI
