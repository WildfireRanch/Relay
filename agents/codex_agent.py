# File: agents/codex_agent.py
# Purpose: Handles code-editing tasks (bugfix, refactor, docstring gen)
# Returns a natural language summary and structured "patch" action with critic analysis
# Exports: codex_agent (CodexAgent instance)

import os
from typing import Dict, Any, Optional, AsyncGenerator
from openai import AsyncOpenAI, OpenAIError

from utils.openai_client import create_openai_client
from utils.patch_utils import validate_patch_format
from core.logging import log_event
from dotenv import load_dotenv
from agents.critic_agent import run_critics
from typing import Dict, Any, Optional, Tuple, AsyncGenerator

load_dotenv()
client = create_openai_client()

class CodexAgent:
    """
    CodexAgent:
        - Handles natural language requests for code editing
        - Returns both a user-friendly summary and a structured patch object
        - All patch actions are validated and critiqued before returning
    """

    async def handle(
        self, query: str, context: str, user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Handles a code-editing request. Generates a patch using GPT-4o, validates, then critiques it.
        Args:
            query: User's natural language request
            context: Relevant code context (usually file contents)
            user_id: (optional) User ID for logging
        Returns:
            Dict with keys:
                - response: summary of patch
                - action: patch dict with critic scores
        """
        if not query or not context:
            raise ValueError("Both 'query' and 'context' must be provided.")

        prompt = self._build_prompt(query, context)

        try:
            completion = await client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a senior software engineer generating code patches from user requests."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            content = completion.choices[0].message.content.strip()
        except OpenAIError as e:
            log_event("codex_agent_error", {"error": str(e), "user_id": user_id})
            raise RuntimeError("Codex agent failed to respond.") from e

        response, action = self._parse_codex_response(content)

        # Patch format validation
        if not validate_patch_format(action):
            log_event("codex_patch_invalid", {"content": content, "user_id": user_id})
            raise ValueError("Invalid patch format returned from Codex.")

        # Critic evaluation
        pseudo_plan = {
            "objective": action.get("reason", ""),
            "steps": [f"Apply patch to {action.get('target_file', 'unknown file')}"],
            "recommendation": "Generated by CodexAgent"
        }

        try:
            critics = await run_critics(pseudo_plan, context)
            action["critics"] = critics
            log_event("codex_patch_critique", {
                "file": action.get("target_file", "unknown"),
                "user_id": user_id,
                "critics": critics
            })
        except Exception as critic_error:
            log_event("codex_critic_fail", {
                "error": str(critic_error),
                "user_id": user_id,
                "target_file": action.get("target_file", "unknown")
            })
            action["critics"] = [{"name": "system", "passes": False, "issues": ["Critic system failed"]}]

        log_event("codex_agent_success", {"action": action, "user_id": user_id})

        return {
            "response": response,
            "action": action
        }

    async def stream(
        self, query: str, context: str, user_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        Streams the patch generation output from the LLM (line by line).
        Args:
            query: User's code-edit request
            context: Relevant code context
            user_id: (optional) User ID
        Yields:
            str: The next output chunk from the model or error.
        """
        if not query or not context:
            yield "[Error] Missing query or context."
            return

        prompt = self._build_prompt(query, context)

        try:
            openai_stream = await client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a senior software engineer generating code patches from user requests."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                stream=True
            )
            async for chunk in openai_stream:
                delta = getattr(chunk.choices[0].delta, "content", None)
                if delta:
                    yield delta
        except Exception as e:
            yield f"[Error] Codex stream failed: {str(e)}"

    def _build_prompt(self, query: str, context: str) -> str:
        """
        Builds a system prompt for the LLM including task and code context.
        """
        return (
            "You will receive a code editing request from a user, along with the relevant code context.\n\n"
            f"Task: {query}\n\n"
            "Code Context:\n"
            "```python\n"
            f"{context}\n"
            "```\n"
            "Respond with a natural language summary, and a JSON object patch in this format:\n"
            '{\n'
            '  "target_file": "<file path>",\n'
            '  "start_line": <int>,\n'
            '  "end_line": <int>,\n'
            '  "replacement": "<new code>",\n'
            '  "reason": "<why this patch>",\n'
            '}\n'
        )

    def _parse_codex_response(self, content: str) -> Tuple[str, Dict[str, Any]]:
        """
        Splits Codex LLM response into summary and action.
        """
        import json, re

        # Try to find the JSON block in the response
        # Summary = everything before JSON, action = parsed JSON object
        json_match = re.search(r'({[\s\S]+})', content)
        if not json_match:
            raise ValueError("Codex did not return a valid patch JSON block.")

        summary = content[:json_match.start()].strip()
        action_json = content[json_match.start():json_match.end()].strip()

        try:
            action = json.loads(action_json)
        except Exception as e:
            raise ValueError(f"Codex patch JSON could not be parsed: {e}\nRaw: {action_json}")

        return summary, action


# Singleton export for app-wide import
codex_agent = CodexAgent()
handle = codex_agent.handle
stream = codex_agent.stream
